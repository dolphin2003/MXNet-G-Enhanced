
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Dark Knowledge in MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to implement Bayesian Dark Knowledge <a name=\"ref-1\"/>[(Korattikara, Rathod, Murphy and Welling, 2015)](#cite-korattikara2015bayesian) in MXNet.\n",
    "\n",
    "In applications like recommendation and control, bayesian treatment of neural networks may be helpful in that we can \n",
    "model the uncertainty of our prediction to avoid overconfident actions <a name=\"ref-2\"/>[(Yeung, Hao and Naiyan, 2015)](#cite-bdl). However, bayesian parameter estimation is non-trivial and much more difficult than a simple point estimation due to the high-dimensionality and non-linearity of neural networks. One way to tackle the problem is the expectation propagation approach in <a name=\"ref-3\"/>[(Hern&aacute;ndez-Lobato and Adams, 2015)](#cite-hernandez2015probabilistic), which relies on a predefined parameteric form of the posterior distribution. The Bayesian Dark Knowledge (BDK) implemented in this notebook is another solution that uses Stochastic Gradient Langevin Dynamics (SGLD) to draw samples from the posterior of the bayesian neural network and fit a student network use these teaching samples. BDK can achieve similar performance as the SGLD teacher while being much faster for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy\n",
    "import time\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "\n",
    "def load_mnist(training_num=50000):\n",
    "    data_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'mnist.npz')\n",
    "    if not os.path.isfile(data_path):\n",
    "        from six.moves import urllib\n",
    "        origin = (\n",
    "            'https://github.com/sxjscience/mxnet/raw/master/example/bayesian-methods/mnist.npz'\n",
    "        )\n",
    "        print 'Downloading data from %s to %s' % (origin, data_path)\n",
    "        context = ssl._create_unverified_context()\n",
    "        urllib.request.urlretrieve(origin, data_path, context=context)\n",
    "        print 'Done!'\n",
    "    dat = numpy.load(data_path)\n",
    "    X = (dat['X'][:training_num] / 126.0).astype('float32')\n",
    "    Y = dat['Y'][:training_num]\n",
    "    X_test = (dat['X_test'] / 126.0).astype('float32')\n",
    "    Y_test = dat['Y_test']\n",
    "    Y = Y.reshape((Y.shape[0],))\n",
    "    Y_test = Y_test.reshape((Y_test.shape[0],))\n",
    "    return X, Y, X_test, Y_test\n",
    "\n",
    "\n",
    "def sample_test_acc(exe, X, Y, label_num=None, minibatch_size=100):\n",
    "    pred = numpy.zeros((X.shape[0], label_num)).astype('float32')\n",
    "    iter = mx.io.NDArrayIter(data=X, label=Y, batch_size=minibatch_size, shuffle=False)\n",
    "    curr_instance = 0\n",
    "    iter.reset()\n",
    "    for batch in iter:\n",
    "        exe.arg_dict['data'][:] = batch.data[0]\n",
    "        exe.forward(is_train=False)\n",
    "        batch_size = minibatch_size - batch.pad\n",
    "        pred[curr_instance:curr_instance + minibatch_size - batch.pad, :] += exe.outputs[0].asnumpy()[:batch_size]\n",
    "        curr_instance += batch_size\n",
    "    correct = (pred.argmax(axis=1) == Y).sum()\n",
    "    total = Y.shape[0]\n",
    "    acc = correct/float(total)\n",
    "    return correct, total, acc\n",
    "\n",
    "\n",
    "def get_executor(sym, ctx, data_inputs, initializer=None):\n",
    "    data_shapes = {k: v.shape for k, v in data_inputs.items()}\n",
    "    arg_names = sym.list_arguments()\n",
    "    aux_names = sym.list_auxiliary_states()\n",
    "    param_names = list(set(arg_names) - set(data_inputs.keys()))\n",
    "    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(**data_shapes)\n",
    "    arg_name_shape = {k: s for k, s in zip(arg_names, arg_shapes)}\n",
    "    params = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    params_grad = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    aux_states = {k: nd.empty(s, ctx=ctx) for k, s in zip(aux_names, aux_shapes)}\n",
    "    exe = sym.bind(ctx=ctx, args=dict(params, **data_inputs),\n",
    "                   args_grad=params_grad,\n",
    "                   aux_states=aux_states)\n",
    "    if initializer != None:\n",
    "        for k, v in params.items():\n",
    "            initializer(k, v)\n",
    "    return exe, params, params_grad, aux_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining some helper functions, we will go on implementing the real-staffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DistilledSGLD(teacher_sym, student_sym,\n",
    "                  teacher_data_inputs, student_data_inputs,\n",
    "                  X, Y, X_test, Y_test, total_iter_num,\n",
    "                  teacher_learning_rate, student_learning_rate,\n",
    "                  teacher_lr_scheduler=None, student_lr_scheduler=None,\n",
    "                  student_optimizing_algorithm='adam',\n",
    "                  teacher_prior_precision=1, student_prior_precision=0.001,\n",
    "                  perturb_deviation=0.001,\n",
    "                  student_initializer=None,\n",
    "                  teacher_initializer=None,\n",
    "                  minibatch_size=100,\n",
    "                  dev=mx.gpu()):\n",
    "    teacher_exe, teacher_params, teacher_params_grad, _ = \\\n",
    "        get_executor(teacher_sym, dev, teacher_data_inputs, teacher_initializer)\n",
    "    student_exe, student_params, student_params_grad, _ = \\\n",
    "        get_executor(student_sym, dev, student_data_inputs, student_initializer)\n",
    "    teacher_label_key = list(set(teacher_data_inputs.keys()) - set(['data']))[0]\n",
    "    student_label_key = list(set(student_data_inputs.keys()) - set(['data']))[0]\n",
    "    teacher_optimizer = mx.optimizer.create('sgld',\n",
    "                                            learning_rate=teacher_learning_rate,\n",
    "                                            rescale_grad=X.shape[0] / float(minibatch_size),\n",
    "                                            lr_scheduler=teacher_lr_scheduler,\n",
    "                                            wd=teacher_prior_precision)\n",
    "    student_optimizer = mx.optimizer.create(student_optimizing_algorithm,\n",
    "                                            learning_rate=student_learning_rate,\n",
    "                                            rescale_grad=1.0 / float(minibatch_size),\n",
    "                                            lr_scheduler=student_lr_scheduler,\n",
    "                                            wd=student_prior_precision)\n",
    "    teacher_updater = mx.optimizer.get_updater(teacher_optimizer)\n",