The following is an overview of MXNet in Chinese. For english readers, please
refer to our [NIPS learnsys paper](http://learningsys.org/papers/LearningSys_2015_paper_1.pdf)

# MXNet设计和实现简介

神经网络本质上是一种语言，我们通过它来表达对应用问题的理解。例如我们用卷积层来表达空间相关性，RNN来表达时间连续性。根据问题的复杂性和信息如何从输入到输出一步步提取，我们将不同大小的层按一定原则连接起来。近年来随着数据的激增和计算能力的大幅提升，神经网络也变得越来越深和大。例如最近几次imagnet竞赛的冠军都使用有数十至百层的网络。对于这一类神经网络我们通常称之为深度学习。从应用的角度而言，对深度学习最重要的是如何方便地表述神经网络，以及如何快速训练得到模型。

对于一个优秀的深度学习系统，或者更广来说优秀的科学计算系统，最重要的是编程接口的设计。他们都采用将一个*领域特定语言(domain specific language)*嵌入到一个主语言中。例如numpy将矩阵运算嵌入到python中。这类嵌入一般分为两种，其中一种嵌入的较浅，其中每个语句都按原来的意思执行，且通常采用*命令式编程(imperative programming)*，其中numpy和Torch就是属于这种。而另一种则用一种深的嵌入方式，提供一整套针对具体应用的迷你语言。这一种通常使用*声明式语言(declarative programing)*，既用户只需要声明要做什么，而具体执行则由系统完成。这类系统包括Caffe，theano和刚公布的TensorFlow。

这两种方式各有利弊，总结如下

- 命令式编程:
    - **如何执行 *a=b+1***: 需要b已经被赋值。立即执行加法，将结果保存在a中。
    - **优点**: 语义上容易理解，灵活，可以精确控制行为。通常可以无缝地和主语言交互，方便地利用主语言的各类算法，工具包，debug和性能调试器。
    - **缺点**: 实现统一的辅助函数和提供整体优化都很困难。
- 声明式编程:
    - **如何执行 *a=b+1***: 返回对应的计算图(computation graph)，我们可以之后对b进行赋值，然后再执行加法运算
    - **优点**: 在真正开始计算的时候已经拿到了整个计算图，所以我们可以做一系列优化来提升性能。实现辅助函数也容易，例如对任何计算图都提供forward和backward函数，对计算图进行可视化，将图保存到硬盘和从硬盘读取。
    - **缺点**: 很多主语言的特性都用不上。某些在主语言中实现简单，但在这里却经常麻烦，例如if-else语句 。debug也不容易，例如监视一个复杂的计算图中的某个节点的中间结果并不简单。

目前现有的系统大部分都采用上两种编程模式的一种。与它们不同的是，MXNet尝试将两种模式无缝的结合起来。在